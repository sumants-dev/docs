---
title: Application Specification
description: Core components of running the application are defined in the following specification.
---

# Specification
## Provider

* `type`- supports `openai`. More to come.
* `default_model`- any of supported models by the provider, i.e. `gpt-3.5-turbo`.
* `api_key` - the API key for your LLM provider.

## Cache
* `cache`- saves oft repeated prompts to limit LLM requests
   * `type`- supports `small_cache`. This cache only supports small prompts for semantic caching.
   * `threshold`- how strict should the cache be on cache hits, from 0 (most lenient) to 1 (most strict).

### Vector Collection
* `vector_collection` is the document store that stores the cache and also is anonymized by our privacy layer.
    * `type`- supports `pgvector`.
    * `conn_str`- the database connection string.
    * `collection_name`- defines the table name to put the prompt cache in.

### Embedder
* `embedder` is a the transformer applied to embed the prompt into vector collection. 
    * `type`- only supports `sentence_transformer` from hugging face.
    * `model`- supports any sentence transformer model.

# Example
```yaml copy
llm:
  provider:
    type: openai
    default_model: gpt-3.5-turbo
    api_key: <put-your-api-key-here>
  cache:
    type: small_cache
    vector_collection:
      type: pgvector
      conn_str: <put-your-api-key-here>
      collection_name: prompt_cache
    embedder:
      type: sentence_transformer
      model: all-MiniLM-L6-v2
```
